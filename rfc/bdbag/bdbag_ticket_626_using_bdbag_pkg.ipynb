{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCA BDBag export proof-of-principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BDBag with two files residing in `azul/rfc/bdbag` (see below). Then compress the bag and upload it to S3. Get a signed URL from the bag on S3 and download the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, bagit, boto3, sys, tempfile, uuid, urllib.request\n",
    "from bdbag import bdbag_api\n",
    "from shutil import rmtree, copy, copyfileobj\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from filecmp import dircmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload data to a FireCloud workspace we need two files, named `participant.tsv` and `sample.tsv`. The first file holds a unique list of donor UUIDs, and the second file the same donor UUIDs with their corresponding specimen UUIDs. The values in these two columns of the table are the composite primary key. The third column in `sample.tsv` holds cell-suspension UUIDs, followed by other columns which I copied from the manifest downloaded from the Explorer of the HCA browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create instance of a `bdbag_api` _bag_ object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp68mhhvwq_bdbag\n"
     ]
    }
   ],
   "source": [
    "original_dir_list = os.listdir()\n",
    "bag_path = tempfile.mkdtemp('_bdbag')  # temporary file name ends with `_bdbag`\n",
    "bag = bdbag_api.make_bag(bag_path)\n",
    "assert os.listdir(os.path.join(bag_path, 'data')) == []  # has data dir. but it's empty\n",
    "print(bag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(bag_path, 'data')\n",
    "\n",
    "def listfiles(basepath):\n",
    "    \"\"\"Return list of file names only, skip directory names found in basepath.\"\"\"\n",
    "    filelist = []\n",
    "    for fname in os.listdir(basepath):\n",
    "        path = os.path.join(basepath, fname)\n",
    "        if os.path.isdir(path):\n",
    "            continue  # skip directories\n",
    "        filelist.append(os.path.basename(path))\n",
    "    return filelist\n",
    "\n",
    "# Copy TSV files from current directory into the data directory of the bag.\n",
    "files = list(filter(lambda x: x.endswith('.tsv'), listfiles(os.getcwd())))\n",
    "for file in files:\n",
    "    copy(file, data_path)\n",
    "assert ['participant.tsv', 'sample.tsv'] == listfiles(data_path)\n",
    "bag = bdbag_api.make_bag(bag_path, update=True)  # write checksums into respective files\n",
    "assert bdbag_api.is_bag(bag_path)\n",
    "bdbag_api.validate_bag(bag_path)\n",
    "assert bdbag_api.check_payload_consistency(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp68mhhvwq_bdbag.zip\n"
     ]
    }
   ],
   "source": [
    "arc_path = bdbag_api.archive_bag(bag_path, 'zip')\n",
    "assert arc_path == bag_path + '.zip'  # arc_path has same basename and .zip\n",
    "print(arc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload zipped bag to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "bucket_name = os.getenv('AZUL_S3_BUCKET')\n",
    "key = str(uuid.uuid4()) + '.zip'\n",
    "if aws_profile is None or bucket_name is None:\n",
    "    sys.exit(\"Check env vars - aborting.\")\n",
    "session = boto3.Session(profile_name=aws_profile)\n",
    "s3 = session.resource('s3')\n",
    "try:\n",
    "    s3.meta.client.upload_file(Filename=arc_path,\n",
    "                               Bucket=bucket_name,\n",
    "                               Key=key)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "rmtree(bag_path, ignore_errors=True)\n",
    "os.remove(arc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download compressed bag from bucket (identified by `key`) from S3\n",
    "The original bag named `bag_path` is in `/tmp` and we previously uploaded it to S3. Here we download that bag and write it to the _current working directory_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = s3.Bucket(bucket_name)\n",
    "for item in my_bucket.objects.all():\n",
    "    if item.key == key:\n",
    "        bucket_name = item.bucket_name  # should be same as above\n",
    "try:\n",
    "    s3.meta.client.download_file(bucket_name, key, './bag.zip')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "assert 'bag.zip' in os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip bag and list its content\n",
    "The bag's name is (still) `bag_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('bag.zip','r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "assert os.path.basename(bag_path) in os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate signed URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = os.getenv('AWS_DEFAULT_REGION')\n",
    "session = boto3.session.Session(region_name=aws_region)\n",
    "s3Client = session.client('s3')\n",
    "params = {'Bucket': bucket_name, 'Key': key}\n",
    "expiration_in_secs = 60\n",
    "url = s3Client.generate_presigned_url('get_object', \n",
    "                                      Params = params, \n",
    "                                      ExpiresIn = expiration_in_secs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file using signed URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.path.basename(bag_path) in os.listdir()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path_original = os.path.basename(bag_path) \n",
    "bag_path = os.path.basename(bag_path)\n",
    "os.rename(bag_path_original, bag_path_original + '_original')\n",
    "try:\n",
    "    with urllib.request.urlopen(url) as response, open('bag_from_url.zip', 'wb') as out_file:\n",
    "        copyfileobj(response, out_file)\n",
    "except HTTPerror as err:\n",
    "    print(err)\n",
    "assert 'bag_from_url.zip' in os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('bag_from_url.zip','r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the original bag with the one downloaded using the signed URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff_files(dcmp):\n",
    "    for name in dcmp.diff_files:\n",
    "        print(\"diff_file %s found in %s and %s\" % (name, dcmp.left,\n",
    "               dcmp.right))\n",
    "    for sub_dcmp in dcmp.subdirs.values():\n",
    "        print_diff_files(sub_dcmp)\n",
    "dcmp = dircmp(bag_path, bag_path_original) \n",
    "assert print_diff_files(dcmp) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdbag_api.cleanup_bag(bag_path)\n",
    "_dirs = [x for x in os.listdir() if x.startswith('tmp')] + [x for x in os.listdir() if x.endswith('.zip')]\n",
    "for _dir in _dirs:\n",
    "    if os.path.isdir(_dir):\n",
    "        rmtree(_dir)\n",
    "    else:\n",
    "        os.remove(_dir)\n",
    "current_dir_list = os.listdir()\n",
    "assert original_dir_list == current_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
